{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install -q datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('SotiriosKastanas/try_groto_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf83d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.features import ClassLabel\n",
    "\n",
    "features = dataset[\"train\"].features\n",
    "column_names = dataset[\"train\"].column_names\n",
    "image_column_name = \"image\"\n",
    "text_column_name = \"tokens\"\n",
    "boxes_column_name = \"bboxes\"\n",
    "label_column_name = \"ner_tags\"\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "\n",
    "\n",
    "label_list = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22']\n",
    "id2label = {k: v for k,v in enumerate(label_list)}\n",
    "label2id = {v: k for k,v in enumerate(label_list)}\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aba551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples(examples):\n",
    "  images = examples[image_column_name]\n",
    "  words = examples[text_column_name]\n",
    "  boxes = examples[boxes_column_name]\n",
    "  word_labels = examples[label_column_name]\n",
    "\n",
    "  encoding = processor(images, words, boxes=boxes, word_labels=word_labels,\n",
    "                       truncation=True, padding=\"max_length\")\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32814986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "\n",
    "# we need to define custom features for `set_format` (used later on) to work properly\n",
    "features = Features({\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': Sequence(feature=Value(dtype='int64')),\n",
    "})\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")\n",
    "eval_dataset = dataset[\"test\"].map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10824d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "return_entity_level_metrics = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d67939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\",\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c873b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30387fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"CORRECT_TEST\",\n",
    "                                  max_steps=13700,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ef637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a5453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af73661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa25b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb675ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncount = 0\\n\\nfor index, row in df_train.iterrows():\\n    bboxes = row[\\'bbox\\']\\n    \\n    bbox_found = False\\n    \\n    for bbox in bboxes:\\n        for element in bbox:\\n            if element > 1000:\\n                bbox_found = True\\n                break  \\n    \\n    if bbox_found:\\n        count += 1\\n\\nprint(\"Total rows with at least one bbox >1000:\", count)\\n\\n\\n\\n\\n\\nfiltered_df = df_train.copy()\\n\\nfor index, row in df_train[::-1].iterrows():\\n    bboxes = row[\\'bbox\\']\\n    \\n    bbox_found = False\\n    \\n    for bbox in bboxes:\\n        for element in bbox:\\n            if element > 1000:\\n                bbox_found = True\\n                break  \\n    \\n    if bbox_found:\\n        filtered_df = filtered_df.drop(index)\\n\\nprint(filtered_df)\\n\\nimport pandas as pd\\n\\ndf_test.reset_index(drop=True, inplace=True)\\n\\ndf_test.index += 11020\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "count = 0\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    bboxes = row['bbox']\n",
    "    \n",
    "    bbox_found = False\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        for element in bbox:\n",
    "            if element > 1000:\n",
    "                bbox_found = True\n",
    "                break  \n",
    "    \n",
    "    if bbox_found:\n",
    "        count += 1\n",
    "\n",
    "print(\"Total rows with at least one bbox >1000:\", count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = df_train.copy()\n",
    "\n",
    "for index, row in df_train[::-1].iterrows():\n",
    "    bboxes = row['bbox']\n",
    "    \n",
    "    bbox_found = False\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        for element in bbox:\n",
    "            if element > 1000:\n",
    "                bbox_found = True\n",
    "                break  \n",
    "    \n",
    "    if bbox_found:\n",
    "        filtered_df = filtered_df.drop(index)\n",
    "\n",
    "print(filtered_df)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_test.index += 11020\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08606ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e1362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2eefb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdf549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
